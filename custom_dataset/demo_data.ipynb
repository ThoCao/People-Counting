{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40312939-62e4-4ca8-9307-b5527e355149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82823/2842199162.py:68: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"config\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  dataset:\n",
      "    dataset_type: CrowdHuman\n",
      "    files_to_download:\n",
      "      CrowdHuman_train01.zip: https://drive.google.com/uc?id=134QOvaatwKdy0iIeNqA_p-xkAhkV4F8Y\n",
      "      CrowdHuman_train02.zip: https://drive.google.com/uc?id=17evzPh7gc1JBNvnW1ENXLy5Kr4Q_Nnla\n",
      "      CrowdHuman_train03.zip: https://drive.google.com/uc?id=1tdp0UCgxrqy1B6p8LkR-Iy0aIJ8l4fJW\n",
      "      CrowdHuman_val.zip: https://drive.google.com/uc?id=18jFI789CoHTppQ7vmRSFEdnGaSQZ4YzO\n",
      "      annotation_train.odgt: https://drive.google.com/uc?id=1UUTea5mYqvlUObsC1Z8CFldHJAtLtMX3\n",
      "      annotation_val.odgt: https://drive.google.com/uc?id=10WIRwu8ju8GRLuCkZ_vT6hnNxs5ptwoL\n",
      "    data_dir: ./crowdhuman\n",
      "    annotations_dir: ./crowdhuman/annotations\n",
      "    image_dir: ./crowdhuman/Images\n",
      "    annotations_file: ./crowdhuman/annotation_train.odgt\n",
      "sample_size: 50\n",
      "batch_size: 1\n",
      "num_workers: 4\n",
      "display_limit: 10\n",
      "\n",
      "Extracting CrowdHuman_train01.zip...\n",
      "Extracting CrowdHuman_train02.zip...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from typing import List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from hydra import initialize, compose\n",
    "import json\n",
    "import gdown\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from data_manager import DatasetManager\n",
    "from my_utils import verify_labels, draw_boxes\n",
    "\n",
    "def main(cfg: Dict[str, Any]) -> None:\n",
    "    dataset_cfg = cfg.dataset  # Access the nested dataset configuration\n",
    "    \n",
    "    # Initialize DatasetManager with the configuration\n",
    "    dataset_manager = DatasetManager(dataset_cfg)\n",
    "    img_ids = dataset_manager.get_img_ids(category_name='person', sample_size=cfg.sample_size)\n",
    "    print(f\"Sampled image IDs: {img_ids}\")\n",
    "    \n",
    "    if not img_ids:\n",
    "        print(\"No images available for sampling.\")\n",
    "        return\n",
    "\n",
    "    dataset = dataset_manager.create_dataset(img_ids, transform=transforms.ToTensor())\n",
    "    dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "\n",
    "    # Save labels in YOLO format\n",
    "    dataset_manager.save_labels(img_ids)\n",
    "\n",
    "    # Verify labels by displaying images with bounding boxes\n",
    "    verify_labels(dataset_cfg)\n",
    "\n",
    "    # Draw bounding boxes on samples and display them using matplotlib\n",
    "    for idx, (images, targets) in enumerate(dataloader):\n",
    "        if idx >= cfg.display_limit:\n",
    "            break\n",
    "\n",
    "        # Convert the tensor image to PIL image\n",
    "        image = transforms.ToPILImage()(images[0])\n",
    "\n",
    "        # Access the bounding boxes\n",
    "        boxes = targets['boxes'].tolist()\n",
    "\n",
    "        # Print debug information\n",
    "        print(f\"Image {idx + 1} has {len(boxes)} bounding boxes.\")\n",
    "        for box in boxes:\n",
    "            print(f\"Bounding box: {box}\")\n",
    "\n",
    "        # Draw the boxes on the image\n",
    "        image_with_boxes = draw_boxes(image, boxes)\n",
    "\n",
    "        # Display the image using matplotlib\n",
    "        plt.imshow(image_with_boxes)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with initialize(config_path=\"config\"):\n",
    "        cfg = compose(config_name=\"config\")\n",
    "        print(OmegaConf.to_yaml(cfg))\n",
    "        main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402d0c3-74f9-4f57-9987-3eac5bfa875c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
